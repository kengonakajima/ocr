import numpy as np
import time
import json
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings('ignore')

# å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from step2_data_loader import OCRDataLoader
from step7_inference import SimplestOCRInference
from step9_improved_nms import ImprovedNMS


class FinalEvaluator:
    """æœ€çµ‚è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model_path='step5_trained_model.h5'):
        print("=== æœ€çµ‚è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ– ===")
        self.ocr = SimplestOCRInference(model_path)
        self.nms = ImprovedNMS()
        self.loader = OCRDataLoader('output')
        print("âœ“ ã‚·ã‚¹ãƒ†ãƒ æº–å‚™å®Œäº†\n")
    
    def evaluate_image(self, image_idx: int, use_improved_nms: bool = True) -> Dict:
        """
        å˜ä¸€ç”»åƒã§è©•ä¾¡ã‚’å®Ÿè¡Œ
        
        Args:
            image_idx: ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            use_improved_nms: æ”¹è‰¯ç‰ˆNMSã‚’ä½¿ç”¨ã™ã‚‹ã‹
        
        Returns:
            è©•ä¾¡çµæœã®è¾æ›¸
        """
        # ç”»åƒã¨Ground Truthã‚’èª­ã¿è¾¼ã¿
        image_array, ground_truth = self.loader.load_image_and_labels(image_idx)
        
        print(f"ç”»åƒ {self.loader.image_files[image_idx]} ã‚’è©•ä¾¡ä¸­...")
        print(f"  Ground Truthæ–‡å­—æ•°: {len(ground_truth)}")
        
        # éƒ¨åˆ†é ˜åŸŸã§ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆè©•ä¾¡ç”¨ã«400Ã—400é ˜åŸŸï¼‰
        eval_region_size = 400
        image_crop = image_array[:eval_region_size, :eval_region_size]
        
        # Ground Truthã‚‚å¯¾å¿œã™ã‚‹é ˜åŸŸã«åˆ¶é™
        gt_in_region = [
            gt for gt in ground_truth 
            if gt['x'] < eval_region_size - 16 and gt['y'] < eval_region_size - 16
        ]
        
        # å®Œå…¨ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œ
        start_time = time.time()
        detections = self.ocr.full_scan(
            image_crop, 
            batch_size=1000, 
            confidence_threshold=0.3  # ä½ã‚ã®é–¾å€¤ã§åºƒãæ¤œå‡º
        )
        scan_time = time.time() - start_time
        
        # NMSé©ç”¨
        if use_improved_nms:
            final_detections = self.nms.apply_nms(
                detections,
                confidence_threshold=0.5,
                distance_threshold=16
            )
        else:
            final_detections = self.ocr.non_maximum_suppression(
                detections, 
                distance_threshold=8
            )
        
        # è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics = self.calculate_metrics(final_detections, gt_in_region)
        metrics['scan_time'] = scan_time
        metrics['detections_before_nms'] = len(detections)
        metrics['detections_after_nms'] = len(final_detections)
        metrics['gt_in_region'] = len(gt_in_region)
        metrics['image_file'] = self.loader.image_files[image_idx]
        
        return metrics, image_crop, final_detections, gt_in_region
    
    def calculate_metrics(self, detections: List[Dict], ground_truth: List[Dict]) -> Dict:
        """
        Precision, Recall, F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        
        Args:
            detections: æ¤œå‡ºçµæœ
            ground_truth: æ­£è§£ãƒ‡ãƒ¼ã‚¿
        
        Returns:
            è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        """
        tp = 0  # True Positives
        fp = 0  # False Positives
        fn = 0  # False Negatives
        
        matched_gt = set()
        matched_detections = []
        
        # å„æ¤œå‡ºã«å¯¾ã—ã¦æœ€ã‚‚è¿‘ã„GTã‚’æ¢ã™
        for det in detections:
            best_match = None
            best_distance = float('inf')
            
            for gt_idx, gt in enumerate(ground_truth):
                if gt_idx in matched_gt:
                    continue
                
                # æ–‡å­—ãŒä¸€è‡´ã—ã€è·é›¢ãŒè¿‘ã„å ´åˆ
                if det['char'] == gt['char']:
                    distance = np.sqrt((det['x'] - gt['x'])**2 + (det['y'] - gt['y'])**2)
                    if distance < 12:  # 12ãƒ”ã‚¯ã‚»ãƒ«ä»¥å†…ãªã‚‰ä¸€è‡´ã¨ã¿ãªã™
                        if distance < best_distance:
                            best_distance = distance
                            best_match = gt_idx
            
            if best_match is not None:
                tp += 1
                matched_gt.add(best_match)
                matched_detections.append((det, ground_truth[best_match]))
            else:
                fp += 1
        
        fn = len(ground_truth) - len(matched_gt)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            'tp': tp,
            'fp': fp,
            'fn': fn,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'matched_pairs': matched_detections
        }
    
    def evaluate_multiple_images(self, image_indices: List[int], 
                               use_improved_nms: bool = True) -> Dict:
        """
        è¤‡æ•°ç”»åƒã§è©•ä¾¡ã‚’å®Ÿè¡Œ
        
        Args:
            image_indices: è©•ä¾¡ã™ã‚‹ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒªã‚¹ãƒˆ
            use_improved_nms: æ”¹è‰¯ç‰ˆNMSã‚’ä½¿ç”¨ã™ã‚‹ã‹
        
        Returns:
            ç·åˆè©•ä¾¡çµæœ
        """
        all_results = []
        total_tp = 0
        total_fp = 0
        total_fn = 0
        total_time = 0
        
        print(f"\n{len(image_indices)}æšã®ç”»åƒã§è©•ä¾¡ã‚’å®Ÿè¡Œ...")
        print("=" * 50)
        
        for i, idx in enumerate(image_indices):
            print(f"\n[{i+1}/{len(image_indices)}] ", end="")
            metrics, _, _, _ = self.evaluate_image(idx, use_improved_nms)
            
            all_results.append(metrics)
            total_tp += metrics['tp']
            total_fp += metrics['fp']
            total_fn += metrics['fn']
            total_time += metrics['scan_time']
            
            print(f"  çµæœ: Precision={metrics['precision']:.3f}, "
                  f"Recall={metrics['recall']:.3f}, F1={metrics['f1']:.3f}")
        
        # ç·åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0
        overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0
        overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) \
                    if (overall_precision + overall_recall) > 0 else 0
        
        avg_metrics = {
            'num_images': len(image_indices),
            'total_tp': total_tp,
            'total_fp': total_fp,
            'total_fn': total_fn,
            'overall_precision': overall_precision,
            'overall_recall': overall_recall,
            'overall_f1': overall_f1,
            'avg_time_per_image': total_time / len(image_indices),
            'individual_results': all_results
        }
        
        return avg_metrics
    
    def visualize_results(self, image_idx: int, save_path: str = 'step10_final_result.png'):
        """
        è©•ä¾¡çµæœã‚’å¯è¦–åŒ–
        
        Args:
            image_idx: è¡¨ç¤ºã™ã‚‹ç”»åƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
            save_path: ä¿å­˜ãƒ‘ã‚¹
        """
        # ç”»åƒã§è©•ä¾¡ã‚’å®Ÿè¡Œ
        metrics, image_crop, detections, ground_truth = self.evaluate_image(image_idx)
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # 1. Ground Truth
        ax = axes[0]
        ax.imshow(image_crop)
        for gt in ground_truth:
            rect = patches.Rectangle((gt['x'], gt['y']), 16, 16,
                                    linewidth=2, edgecolor='green', facecolor='none')
            ax.add_patch(rect)
            ax.text(gt['x'], gt['y']-2, gt['char'], color='green', 
                   fontsize=8, weight='bold')
        ax.set_title(f'Ground Truth ({len(ground_truth)} chars)')
        ax.axis('off')
        
        # 2. Detections
        ax = axes[1]
        ax.imshow(image_crop)
        for det in detections:
            rect = patches.Rectangle((det['x'], det['y']), 16, 16,
                                    linewidth=2, edgecolor='red', facecolor='none')
            ax.add_patch(rect)
            ax.text(det['x'], det['y']-2, det['char'], color='red', 
                   fontsize=8, weight='bold')
        ax.set_title(f'Detections ({len(detections)} chars)')
        ax.axis('off')
        
        # 3. Metrics
        ax = axes[2]
        ax.axis('off')
        metrics_text = f"""
Performance Metrics:
-------------------
True Positives:  {metrics['tp']}
False Positives: {metrics['fp']}
False Negatives: {metrics['fn']}

Precision: {metrics['precision']:.3f}
Recall:    {metrics['recall']:.3f}
F1 Score:  {metrics['f1']:.3f}

Processing Time: {metrics['scan_time']:.2f}s
Before NMS: {metrics['detections_before_nms']}
After NMS:  {metrics['detections_after_nms']}
        """
        ax.text(0.1, 0.5, metrics_text, fontsize=12, family='monospace',
               verticalalignment='center')
        ax.set_title('Evaluation Results')
        
        plt.suptitle(f'Final OCR Evaluation - {metrics["image_file"]}')
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"\nè©•ä¾¡çµæœã‚’ '{save_path}' ã«ä¿å­˜ã—ã¾ã—ãŸ")


def main():
    print("="*60)
    print("ã‚¹ãƒ†ãƒƒãƒ—10: æœ€çµ‚è©•ä¾¡ã®å®Ÿæ–½")
    print("="*60)
    
    # è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
    evaluator = FinalEvaluator()
    
    # ãƒ†ã‚¹ãƒˆç”»åƒã‚»ãƒƒãƒˆã‚’é¸æŠï¼ˆå­¦ç¿’ã«ä½¿ç”¨ã—ã¦ã„ãªã„ç”»åƒï¼‰
    test_indices = list(range(400, 410))  # 10æšã§ãƒ†ã‚¹ãƒˆ
    
    # è¤‡æ•°ç”»åƒã§è©•ä¾¡
    print("\n=== è¤‡æ•°ç”»åƒã§ã®è©•ä¾¡ ===")
    avg_metrics = evaluator.evaluate_multiple_images(test_indices, use_improved_nms=True)
    
    # çµæœã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    print("\n" + "="*60)
    print("è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    print(f"è©•ä¾¡ç”»åƒæ•°: {avg_metrics['num_images']}")
    print(f"ç·True Positives: {avg_metrics['total_tp']}")
    print(f"ç·False Positives: {avg_metrics['total_fp']}")
    print(f"ç·False Negatives: {avg_metrics['total_fn']}")
    print("-"*40)
    print(f"å…¨ä½“Precision: {avg_metrics['overall_precision']:.4f}")
    print(f"å…¨ä½“Recall: {avg_metrics['overall_recall']:.4f}")
    print(f"å…¨ä½“F1ã‚¹ã‚³ã‚¢: {avg_metrics['overall_f1']:.4f}")
    print(f"å¹³å‡å‡¦ç†æ™‚é–“: {avg_metrics['avg_time_per_image']:.2f}ç§’/ç”»åƒ")
    
    # F1ã‚¹ã‚³ã‚¢ã®åˆ¤å®š
    print("\n" + "="*60)
    if avg_metrics['overall_f1'] >= 0.5:
        print("âœ“ æˆåŠŸ: F1ã‚¹ã‚³ã‚¢ãŒ0.5ä»¥ä¸Šã‚’é”æˆã—ã¾ã—ãŸï¼")
        print(f"  é”æˆå€¤: {avg_metrics['overall_f1']:.4f}")
    else:
        print("â–³ æ”¹å–„å¿…è¦: F1ã‚¹ã‚³ã‚¢ãŒ0.5æœªæº€ã§ã™")
        print(f"  ç¾åœ¨å€¤: {avg_metrics['overall_f1']:.4f}")
        print("\næ”¹å–„æ¡ˆ:")
        print("  1. ã‚ˆã‚Šå¤šãã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
        print("  2. ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é©ç”¨")
        print("  3. ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ”¹è‰¯")
        print("  4. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´")
    
    # ä»£è¡¨çš„ãªç”»åƒã§å¯è¦–åŒ–
    print("\nä»£è¡¨ç”»åƒã§ã®è©³ç´°è©•ä¾¡...")
    evaluator.visualize_results(test_indices[0])
    
    # çµæœã‚’JSONä¿å­˜
    save_data = {
        'test_indices': test_indices,
        'overall_metrics': {
            'precision': float(avg_metrics['overall_precision']),
            'recall': float(avg_metrics['overall_recall']),
            'f1': float(avg_metrics['overall_f1']),
            'avg_time': float(avg_metrics['avg_time_per_image'])
        },
        'counts': {
            'tp': int(avg_metrics['total_tp']),
            'fp': int(avg_metrics['total_fp']),
            'fn': int(avg_metrics['total_fn'])
        },
        'individual_results': [
            {
                'image': r['image_file'],
                'precision': float(r['precision']),
                'recall': float(r['recall']),
                'f1': float(r['f1']),
                'tp': int(r['tp']),
                'fp': int(r['fp']),
                'fn': int(r['fn'])
            }
            for r in avg_metrics['individual_results']
        ]
    }
    
    with open('step10_evaluation_results.json', 'w') as f:
        json.dump(save_data, f, indent=2)
    
    print("\nè©³ç´°çµæœã‚’ 'step10_evaluation_results.json' ã«ä¿å­˜ã—ã¾ã—ãŸ")
    
    print("\n" + "="*60)
    print("ã‚¹ãƒ†ãƒƒãƒ—10å®Œäº†")
    print("="*60)
    print("âœ“ ãƒ†ã‚¹ãƒˆç”»åƒã§ã®æ–‡å­—æ¤œå‡ºã‚’å®Ÿè¡Œ")
    print("âœ“ æ¤œå‡ºçµæœã¨Ground Truthã‚’æ¯”è¼ƒ")
    print("âœ“ ç²¾åº¦ãƒ»å†ç¾ç‡ãƒ»F1ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—")
    if avg_metrics['overall_f1'] >= 0.5:
        print("âœ“ F1ã‚¹ã‚³ã‚¢ãŒ0.5ä»¥ä¸Šã‚’é”æˆ")
    else:
        print(f"â–³ F1ã‚¹ã‚³ã‚¢: {avg_metrics['overall_f1']:.4f} (ç›®æ¨™: 0.5ä»¥ä¸Š)")
    
    print("\nğŸ‰ ã™ã¹ã¦ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    
    return avg_metrics


if __name__ == "__main__":
    main()